---
title: "The Bike-sharing rental process"
output: word_document
---
 
1. Problem description

Bike sharing systems are new generation of traditional bike rentals where whole process from membership, rental and return has become automatic. Through these systems, user can easily rent a bike from a position and return at another position. Currently, there are about over 500 bike-sharing programs around the world which is composed of over 500 thousand bicycles. Today, there exists great interest in these systems due to their important role in traffic, environmental and health issues. 

Apart from interesting real world applications of bike sharing systems, the characteristics of data being generated by these systems make them attractive for the research. Opposed to other transport services such as bus or subway, the duration of travel, departure and arrival position is explicitly recorded in these systems. This feature turns bike sharing system into a virtual sensor network that can be used for sensing mobility in the city. Hence, it is expected that most of important events in the city could be detected via monitoring these data. 

2. Analysis process 

2.1 Data collection and description

The source for the dataset is the Capital Bikeshare system, USA. Bike-sharing rental process is highly correlated to the environmental and seasonal settings. For instance, weather conditions, precipitation, day of week, season, hour of the day, etc. can affect the rental behaviors. The core data set is related to the two-year historical log corresponding to years 2011 and 2012 from Capital Bikeshare system, Washington D.C., USA which is publicly available in http://capitalbikeshare.com/system-data. We aggregated the data on two hourly and daily basis and then extracted and added the corresponding weather and seasonal information. Weather information are extracted from http://www.freemeteo.com. The prediction of bike rental count hourly or daily based on the environmental and seasonal settings. 

2.2 Understanding DataSet:

-- holiday: This is a categorical variable with levels (extracted from http://dchr.dc.gov/page/holiday-schedule).

--weathersit: This is a categorical variable with levels [1,2,3,4]

--temp : Normalized temperature in Celsius. The values are divided to 41 (max)

-- hum: Normalized humidity. The values are divided to 100 (max)

-- windspeed: Normalized wind speed. The values are divided to 67 (max)

--cnt: count of total rental bikes including both casual and registered. This is the response variable

--season: seasons (1: spring, 2: summer, 3: fall, 4: winter)

--yr: year (0: 2011, 1: 2012)

--Month: month (1 to 12)

--Casual: count of casual users

--registered: count of registered users

--workingday: factor variable with levels [0,1]

2.3 Data Analysis with Plots

Here we have three response variables, cnt, registered and casual. 

```{r include=FALSE}
require(corrplot)
require(ggplot2)
library(MASS)
require(leaps)
require(HH)
day <- read.csv("C:/Users/vasir/Desktop/day.csv")
M <- cor(day)

```


```{r echo=FALSE}

ggplot(day, aes(as.factor(season), cnt)) + geom_boxplot()
ggplot(day, aes(as.factor(weathersit), registered)) + geom_boxplot()
ggplot(day, aes(as.factor(yr), cnt)) + geom_boxplot()
ggplot(day, aes(as.factor(workingday), cnt)) + geom_boxplot()
ggplot(day, aes(as.factor(holiday), cnt)) + geom_boxplot()

```



```{r echo=FALSE}

corrplot(M, type="lower",method = "number")

```

From the above corelation plot we see that the 'temp','hum' and 'windspeed' are good predictors for fitting the model.


```{r include=FALSE}

lm.model<-lm(cnt ~ as.factor(holiday) + as.factor(workingday) + temp + hum + windspeed,data=day)

lm.model1<-lm(cnt ~ as.factor(holiday) + as.factor(workingday) + temp + hum + windspeed + as.factor(season),data=day)

X=(day$temp)^2

lm.model4<-lm(cnt ~ as.factor(holiday) + as.factor(workingday) + temp + X +hum + windspeed + as.factor(season),data=day)

lm.model2<-lm(cnt ~ as.factor(holiday) + temp + hum + windspeed + as.factor(season),data=day)

y=log(day$cnt)

lm.model3<-lm(y ~ as.factor(holiday) + as.factor(workingday) +temp + hum + windspeed + as.factor(season),data=day)

lm.model5<-lm(cnt ~ as.factor(holiday) + as.factor(workingday) + temp + hum + windspeed + as.factor(season)+ as.factor(weathersit),data=day)

lm.model6<-lm(cnt ~ as.factor(holiday) + temp + hum + windspeed + as.factor(season)+ as.factor(yr),data=day)

data=cbind(mean(vif(lm.model)),mean(vif(lm.model1)),mean(vif(lm.model2)),mean(vif(lm.model3)),mean(vif(lm.model3)),mean(vif(lm.model5)),mean(vif(lm.model6)))

lev=hat(model.matrix(lm.model6))

day[lev>0.025,c(1)]

point=c(17,50,52,69,105,150,185,248,283,315,328,360,367,381,416,433,472,514,551,612,647,682,692,725)

r=rstudent(lm.model6)

cook=cooks.distance(lm.model6)
 
plot(cook)

day2=day[c(-69,-692),]

lm.model7<-lm(cnt ~ as.factor(holiday) + temp + hum + windspeed + as.factor(season)+ as.factor(yr),data=day2)

```


```{r include=FALSE}

mean=mean(day$cnt)
variance=var(day$cnt)
ratio=variance/mean

glm.model1<-glm(cnt ~ as.factor(holiday) + as.factor(workingday) + temp + hum + windspeed + as.factor(season),family='poisson',data=day)

glm.model2<-glm(cnt ~ as.factor(holiday) + as.factor(workingday) + temp + hum + windspeed + as.factor(season),family=quasipoisson(link="log"),data=day)

glm.model3<-glm.nb(cnt ~ as.factor(holiday) + as.factor(workingday) + temp + hum + windspeed + as.factor(season),data=day)

```

```{r echo=FALSE}

summary(lm.model)
ggplot(lm.model,aes(lm.model$fitted.values,lm.model$residuals))+geom_point()

```

From the above summary statistics the p-value is not significant for workingday. And the plot shows that there is no constant variance. So, we will fit another model by considering the 'season' column in the dataset.

```{r echo=FALSE}

summary(lm.model1)
ggplot(lm.model1,aes(lm.model1$fitted.values,lm.model1$residuals))+geom_point()
ggplot(lm.model1,aes(day$holiday,lm.model1$residuals))+geom_point()
ggplot(lm.model1,aes(day$workingday,lm.model1$residuals))+geom_point()
ggplot(lm.model1,aes(day$temp,lm.model1$residuals))+geom_point()
ggplot(lm.model1,aes(day$hum,lm.model1$residuals))+geom_point()
ggplot(lm.model1,aes(day$windspeed,lm.model1$residuals))+geom_point()

```

We still see some curvature in the residual vs fitted values plot and the p-value for working day is not significant. From the residual vs temp plot we see there is some curvature. So, adding the higher order term for temp and fitting the model again.

```{r echo=FALSE}

summary(lm.model4)
ggplot(lm.model4,aes(lm.model4$fitted.values,lm.model1$residuals))+geom_point()

```

The residuals Vs fitted values we see there is increasing constant variance. The standard error value for temp increased alot eventhough p-value is sognificant. So, we will remove the quadratic term from the model as we need standard error 's' to be small. As p-value is not significant for 'workingday' we fitted the model by removing the 'working day'.

```{r echo=FALSE}

summary(lm.model2)
ggplot(lm.model,aes(lm.model2$fitted.values,lm.model2$residuals))+geom_point()


```

As we see from the above plot, still we have some curvature in the residual vs fitted values.So, applied the box-cox transformation for the response variable and fitting the model by adding the 'workingday' column.

```{r echo=FALSE}

summary(lm.model3)
ggplot(lm.model3,aes(lm.model3$fitted.values,lm.model3$residuals))+geom_point()
qqnorm(lm.model1$residuals)
qqline(lm.model1$residuals)
qqnorm(lm.model3$residuals)
qqline(lm.model3$residuals)

```

Eventhorugh transformation reduced the standard error values, the plot between fitted values and residuals have little curvature. Comparing the QQplots for both model1 by adding the season column and model3 for which we have transformed the response variable, we can see that model1 is fitting better than model3.

By considering the 'yr' and fitted the model.

```{r echo=FALSE}

summary(lm.model6)
ggplot(lm.model6,aes(lm.model6$fitted.values,lm.model6$residuals))+geom_point()
qqnorm(lm.model6$residuals)
qqline(lm.model6$residuals)

```

From the summary of above model, p-values are significant and 'S' is small and p-values are significant.

Finally, the model6 is best fit for predicting the values.

```{r echo=FALSE}

`colnames<-`(data,c("model","model1","model2","model3","model4","model5","model6"))

```

The mean values for VIF for all models is less than 10. So, all models are significant. The R-squared is high for model7 compared to other models and Standard Error is less compared to other models.

Leverages:

```{r echo=FALSE}

plot(lev)

day[lev>0.025,c(1)]

length(day[lev>0.025,c(1)])

```

The maximum leverage value should be (2*(k+1)/n) which is approximately 0.025. There are around 24 observations having higher leverage than all other points.

An influential point is one if removed from the data would significantly change the fit. An influential point may either be an outlier or have large leverage, or both, but it will tend to have at least one of those properties. Cook's distance is a commonly used influence measure that combines these two properties.

```{r include=FALSE}

par(mfrow=c(4,6))

```

Plotting all the 24 observations for cook's distance.

```{r echo=FALSE}

for(i in point){ 
  cook=cooks.distance(lm.model6) 
  plot(cook) 
  points(i,cook[i],col='red')
  }

```

From the above plots we can say that, the observation 69 and 692  are influential. Between 69 and 692, 692 observation is most influential, so removing that data and fitting the model again.

```{r echo=FALSE}

summary(lm.model7)

ggplot(lm.model7,aes(lm.model7$fitted.values,lm.model7$residuals))+geom_point()

qqnorm(lm.model7$residuals)
qqline(lm.model7$residuals)

hist(lm.model7$residuals)

```

From the residuals vs fitted values we see there is little curvature in the model. But the qqplot and histogram for residuals we see that the model is good fit for the data.

```{r echo=FALSE}

ggplot(day,aes(cnt))+geom_histogram(binwidth = 200)

```

Other Distributions tested on Data Set:

From the histogram above, it seems that the number of total rented bikes follow a nearly normal distribution. Many studies have revealed that Poisson distribution can be used to analyze count data. The mean and variance of Poisson distribution are the same, and when the mean is getting larger, Poisson distribution approximates a normal distribution.Poisson regression is good for modeling count data. It assumes the response variable follows Poisson distribution and can be explained by linear combination of the explanatory variables.

```{r echo=FALSE}

cbind(mean,variance,ratio)

```

We can see there is hufge difference between the mean and variance, So, clearly it doesnot follow the standard poisson distribution. For the count data, if the variance is larger than mean, it means the data has over dispersion.

We first fit the standard poisson distribution.

```{r echo=FALSE}

summary(glm.model1)

```

Even though p values are significant the Residual deviance and degree of freedom suggest that model doesn't fit the data well. The reason for that is probably because of over dispersion in the dataset.

To resolve the over-dispersion issue we use 2 models, namely

1) Quassi-Poisson Regression
2) Negative-binomial

Quasi-Poisson Regression:

Quasi-Poisson is remedy for over dispersion in Poisson model. It estimates the scale parameter as well, and fixes the estimated standard error. It gives us more conservation results.

```{r echo=FALSE}

summary(glm.model2)

```

Comparing the two tables above, we can see that the estimated standard errors in Quasi-Poisson model are larger than that in Poisson model. And the dispersion parameter given by Quasi-Poisson model is 422.3375, which means there is indeed over dispersion in the data set and Poisson model underestimated the standard errors.

Negative-Binomial model:

Negative Binomial model is also very useful for modeling count variable, especially for the data set with over dispersion. Compared to Poisson model, it contains an extra parameter theta, which is the parameter of multiplicative random effect.

```{r echo=FALSE}

summary(glm.model3)
#chi square test
pchisq(1682.4-743.5,df=(729-719),lower.tail = FALSE)

```

The Chi-Square test p-value for the negative binomial is less than 0.05. So, negative binomial model is also fitting the model correctly.

Conclusion:

The count of total rental bikes depends on factors like the temperature, the windspeed, the type of weather, holiday, working day and humidity. We haven't considered the weather type for linear regression models as the weather type is already covered using the parameters temp, Humidity and Windspeed. For linear regression models, box-cox transformation has been applied for the best fit of the model. Eventhough, the R-squared and p-values are significant the plot for residuals and fitted values has little curvature.

Since the response is a count variable with high values we have chosen the Negative Binomial method. Based on the dispersion parameter, the maximum likelihood, the residual deviance, p values, standard error, the AIC values, the theta values, and the chi square test are significant but the log likehood value is not significant. So, we conclude that this model suits the data better than the Poisson model but not better than linear regression model.


Contribution:

Divya : 85%
Harsha: 15%
